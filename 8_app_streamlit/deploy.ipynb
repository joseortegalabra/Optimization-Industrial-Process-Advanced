{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea04d11-b156-4dff-ac80-08ab3e80c79b",
   "metadata": {},
   "source": [
    "# Deploy streamlit app into a cloud run (gcp)\n",
    "\n",
    "**Following the ideas of:**\n",
    "- deploy streamlit app in cloud run: https://medium.com/@faizififita1/how-to-deploy-your-streamlit-web-app-to-google-cloud-run-ba776487c5fe\n",
    "- deploy streamlit app into google app engine: https://dev.to/whitphx/how-to-deploy-streamlit-apps-to-google-app-engine-407o\n",
    "- deploy a flask app into a cloud run: my previous codes\n",
    "    - Create Dockerfile (an example Dockerfile can be found at: https://firebase.google.com/docs/hosting/cloud-run?hl=es-419#python)\n",
    "\n",
    "\n",
    "**Steps:**\n",
    "- Create a sript with codes\n",
    "- Create a .env file with environment variables that you don't want to expose\n",
    "- Define the parameters to deploy\n",
    "- Run codes that do diferents steps to deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc770c07-ce2a-492f-97ac-bd9fb8aaa545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03c6054b-c6e1-4a49-b788-156a92e3a057",
   "metadata": {},
   "source": [
    "## I) INTRODUCTION\n",
    "\n",
    "These codes could be run in the Google SDK console, as well as run on a notebook.\n",
    "For a data scientist who is not specialized in devops practices, using a notebook is more intuitive to use\n",
    "\n",
    "#### Previous steps\n",
    "- Stop in the root folder of the application (this notebook is created in the root)\n",
    "- Have scripts created to contain and upload to cloud run\n",
    "\n",
    "#### Important information\n",
    "**To run a console command in a Jupyter notebook and the python variables stored in the notebook can also be passed to the command, you must use the peso sign ($) and not use the assignment command (=)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c5998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646304fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef96bd-3a6c-48df-8078-6efaf5b6c09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bf9ae26",
   "metadata": {},
   "source": [
    "## II) INITIALIZE READ .ENV WITH ENV VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "338a3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv # package used in jupyter notebook to read the variables in file .env\n",
    "\n",
    "\"\"\" get env variable from .env \"\"\"\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "\"\"\" Read env variables and save it as python variable \"\"\"\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_GCP\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e91a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c42cc73c-8063-4e9f-96c7-ae5343c55c53",
   "metadata": {},
   "source": [
    "## III) DEFINE PARAMETERS TO DEPLOY APP INTO A CLOUD RUN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4bd3e5-8975-45c0-abbc-a5eea58acfd0",
   "metadata": {},
   "source": [
    "### Step 0: Connect to GCP project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d1563e-5625-4963-952d-4c6468d261c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db232b7-2239-4ccc-b8f4-7c313b09cd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d690650-e360-4d14-9c69-e792e32eb0f1",
   "metadata": {},
   "source": [
    "### Step 1: Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90fe41fe-0fdc-4999-b2f8-7e225cfdb5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARARAMETERS\n",
    "\n",
    "# general gcp\n",
    "REGION = 'us-east1'\n",
    "\n",
    "# name of the repo in artifact registry where will be saved docker images\n",
    "NAME_REPO = 'repo-gurobi-optimization-bleaching-sf2-advanced'\n",
    "FORMAT_REPO = 'docker'\n",
    "DESCRIPTION_REPO = \"repo web app con modelo de optimizaci√≥n de gurobi sf2 advanced\"\n",
    "\n",
    "# name of the docker image saved in docker repo in artifact registry\n",
    "NAME_IMAGE = 'gurobi-optimization-bleaching-sf2-advanced'\n",
    "\n",
    "# name of cloud run where the app web will be located\n",
    "NAME_CLOUD_RUN = 'gurobi-optimization-bleaching-sf2-advanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839eb1d7-f6ba-4d98-ba9b-2490e1cc66a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0591b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4eda8-348f-439f-9413-adba04c635ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62e1c63e",
   "metadata": {},
   "source": [
    "# IV) Upload a docker image with the codes of the app in Artifact Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f0053-ca26-44cf-ac37-de8928fe4bc9",
   "metadata": {},
   "source": [
    "- Artifact registry is the replacement for container registry and recommended by google. The only difference is that the image is saved in this new service and you need to run another command\n",
    "\n",
    "- **Additionally, every time a new image is uploaded to the artifact registry (rerun the corresponding gcloud command), it receives the latest tag and is the one used to create/update the created cloud run**\n",
    "\n",
    "- Uploading the image to the artifact Registry requires more steps than uploading it to the container registry\n",
    "\n",
    "- Cloud build integration documentation with Artifact Registry: https://cloud.google.com/artifact-registry/docs/configure-cloud-build?hl=es-419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072af20-fc4b-4bd0-87f3-eac65495a710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fccab59b-8d26-4e56-ac6f-61d71014890d",
   "metadata": {},
   "source": [
    "### Step 1. Create repository in artifact registry (if it does not exist)\n",
    "- Unlike container registry which was automatic, in artifact registry you have to create it. **If the repo already exists the gcloud command return an error but doesn't stop de execution of the notebook**\n",
    "\n",
    "- A repo is created which can have multiple images and each one have different versions\n",
    "\n",
    "- Documentation: create repo in artifact registry: https://cloud.google.com/artifact-registry/docs/repositories/create-repos#gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceae1f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create request issued for: [repo-gurobi-optimization-bleaching-sf2-advanced]\n",
      "Check operation [projects/cmpc-innovation-cd4ml-test/locations/us-east1/operations/bbc4f7a6-b94d-419b-88fc-24be65031da0] for status.\n"
     ]
    }
   ],
   "source": [
    "# create repo artifact registry\n",
    "! gcloud artifacts repositories create $NAME_REPO \\\n",
    "--repository-format $FORMAT_REPO \\\n",
    "--location $REGION \\\n",
    "--description \"$DESCRIPTION_REPO\" \\\n",
    "--async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040f26c-e1a6-400d-a358-5502c822f932",
   "metadata": {},
   "source": [
    "### Step 2: Set up a Docker build\n",
    "\n",
    "It is necessary to create a **yaml** with the configuration to build the docker image in Artifact Registry.\n",
    "\n",
    "It has the following form\n",
    "\n",
    "<code>\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "   args: [ 'build', '-t', '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}', '.' ]\n",
    "images:\n",
    "- '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}'\n",
    "<code>\n",
    "    \n",
    "\n",
    "**This is a GENERIC FILE that can be recycled because it is parameterized to work with any docker repo in the artifact registry**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dcace8-1a0f-4833-89ab-544bfc2aa7c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f13b0ae",
   "metadata": {},
   "source": [
    "--> Running the following line of code creates a yaml file with the desired configuration.\n",
    "\n",
    "Documentation: https://stackabuse.com/reading-and-writing-yaml-to-a-file-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98471485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# create a python diccionary with the content of the yaml cloudbuild generic\n",
    "\n",
    "dict_python_yaml_cloudbuild = {'steps': [{'name': 'gcr.io/cloud-builders/docker',\n",
    "   'args': ['build', '-t', '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}', '.']}],\n",
    " 'images': ['${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}']}\n",
    "\n",
    "# save dicctionary in yaml format\n",
    "with open(r'cloudbuild.yaml', 'w') as file:\n",
    "    documents = yaml.dump(dict_python_yaml_cloudbuild, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e475e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b267e53",
   "metadata": {},
   "source": [
    "### Step 3: Create Dockerfile\n",
    "Like the previous step, in this case you need to create the Dockerfile to be able to upload the image to the Artifact Registry. Typically, the dockerfile is created manually.\n",
    "\n",
    "In this example, the content of the dockerfile is defined within a parameterized string, which allows it to be a transversal file and applicable to any deploy in a cloud run\n",
    "\n",
    "Generally, most of the time, **the Dockerfile does not need to be modified unless you want to change the python vers**ion. So by running the following code you obtain the Dockerfile of the web app\n",
    "\n",
    "https://prnt.sc/uwBOFChK8QU8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0e2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A STRING THAT REPRESENT THE DOCKER FILE\n",
    "\n",
    "#old version with requirements file inside the script\n",
    "# string_dockerfile = '''\n",
    "# FROM python:3.10\n",
    "# EXPOSE 8080\n",
    "# WORKDIR /app\n",
    "# COPY . ./\n",
    "# RUN pip install streamlit gunicorn\n",
    "# ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8080\", \"--server.address=0.0.0.0\"]\n",
    "# '''\n",
    "\n",
    "\n",
    "string_dockerfile = '''\n",
    "FROM python:3.10\n",
    "EXPOSE 8080\n",
    "WORKDIR /app\n",
    "COPY . ./\n",
    "RUN pip install -r requirements.txt\n",
    "ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8080\", \"--server.address=0.0.0.0\"]\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc57cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar dockerfile\n",
    "with open('Dockerfile', 'w') as file:\n",
    "    file.write(string_dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b43e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15903a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba335ac5",
   "metadata": {},
   "source": [
    "### Step 4: Containerize (docker image) web app codes using cloud build and upload them to artifact registry\n",
    "- In this step, a docker image is created with the necessary codes for the web app and then this image is uploaded to Artifact Registry (using as a base the \"cloudbuild.yaml\" file that calls the \"Dockerfile\", created in the steps previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f993074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"daff1f96-2892-4c8a-8a80-fb0804c8b19e\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://cmpc-innovation-cd4ml-test_cloudbuild/source/1708917721.278808-38049e5654ca4763a6e8e8d4464a2e66.tgz#1708917722531749\n",
      "Copying gs://cmpc-innovation-cd4ml-test_cloudbuild/source/1708917721.278808-38049e5654ca4763a6e8e8d4464a2e66.tgz#1708917722531749...\n",
      "/ [0 files][    0.0 B/123.8 KiB]                                                \n",
      "/ [1 files][123.8 KiB/123.8 KiB]                                                \n",
      "\n",
      "Operation completed over 1 objects/123.8 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  604.7kB\n",
      "\n",
      "\n",
      "Step 1/6 : FROM python:3.10\n",
      "3.10: Pulling from library/python\n",
      "7bb465c29149: Pulling fs layer\n",
      "2b9b41aaa3c5: Pulling fs layer\n",
      "49b40be4436e: Pulling fs layer\n",
      "c558fac597f8: Pulling fs layer\n",
      "11402150a57e: Pulling fs layer\n",
      "78d5a3440207: Pulling fs layer\n",
      "833a11e16f36: Pulling fs layer\n",
      "e81a4b56efff: Pulling fs layer\n",
      "c558fac597f8: Waiting\n",
      "11402150a57e: Waiting\n",
      "78d5a3440207: Waiting\n",
      "833a11e16f36: Waiting\n",
      "e81a4b56efff: Waiting\n",
      "2b9b41aaa3c5: Download complete\n",
      "7bb465c29149: Verifying Checksum\n",
      "7bb465c29149: Download complete\n",
      "49b40be4436e: Verifying Checksum\n",
      "49b40be4436e: Download complete\n",
      "11402150a57e: Download complete\n",
      "833a11e16f36: Verifying Checksum\n",
      "833a11e16f36: Download complete\n",
      "78d5a3440207: Verifying Checksum\n",
      "78d5a3440207: Download complete\n",
      "e81a4b56efff: Verifying Checksum\n",
      "e81a4b56efff: Download complete\n",
      "c558fac597f8: Verifying Checksum\n",
      "c558fac597f8: Download complete\n",
      "7bb465c29149: Pull complete\n",
      "2b9b41aaa3c5: Pull complete\n",
      "49b40be4436e: Pull complete\n",
      "c558fac597f8: Pull complete\n",
      "11402150a57e: Pull complete\n",
      "78d5a3440207: Pull complete\n",
      "833a11e16f36: Pull complete\n",
      "e81a4b56efff: Pull complete\n",
      "Digest: sha256:b54e76c629a98430ac9c92e4f6bddeb672396a895b44a85022d12ee2f7239144\n",
      "Status: Downloaded newer image for python:3.10\n",
      " ---> 22546fe66182\n",
      "Step 2/6 : EXPOSE 8080\n",
      " ---> Running in 58e9f907ceb3\n",
      "Removing intermediate container 58e9f907ceb3\n",
      " ---> b10ab34841f1\n",
      "Step 3/6 : WORKDIR /app\n",
      " ---> Running in 86b62cda61be\n",
      "Removing intermediate container 86b62cda61be\n",
      " ---> ad8e2ceee987\n",
      "Step 4/6 : COPY . ./\n",
      " ---> 5b6e538331e3\n",
      "Step 5/6 : RUN pip install -r requirements.txt\n",
      " ---> Running in 6777c72a535f\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.31.1-py2.py3-none-any.whl (8.4 MB)\n",
      "     ???????????????????????????????????????? 8.4/8.4 MB 70.5 MB/s eta 0:00:00\n",
      "Collecting gunicorn\n",
      "  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
      "     ???????????????????????????????????????? 80.2/80.2 kB 16.3 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "     ??????????????????????????????????????? 13.0/13.0 MB 100.4 MB/s eta 0:00:00\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "     ???????????????????????????????????????? 11.6/11.6 MB 94.3 MB/s eta 0:00:00\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "     ?????????????????????????????????????? 294.9/294.9 kB 38.5 MB/s eta 0:00:00\n",
      "Collecting plotly\n",
      "  Downloading plotly-5.19.0-py3-none-any.whl (15.7 MB)\n",
      "     ???????????????????????????????????????? 15.7/15.7 MB 94.0 MB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "     ???????????????????????????????????????? 18.2/18.2 MB 87.4 MB/s eta 0:00:00\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "     ???????????????????????????????????????? 12.1/12.1 MB 91.9 MB/s eta 0:00:00\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "     ?????????????????????????????????????? 250.0/250.0 kB 30.8 MB/s eta 0:00:00\n",
      "Collecting gurobipy\n",
      "  Downloading gurobipy-11.0.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (13.4 MB)\n",
      "     ??????????????????????????????????????? 13.4/13.4 MB 100.7 MB/s eta 0:00:00\n",
      "Collecting gurobipy-pandas\n",
      "  Downloading gurobipy_pandas-1.1.1-py3-none-any.whl (19 kB)\n",
      "Collecting gurobi-machinelearning\n",
      "  Downloading gurobi_machinelearning-1.4.0-py3-none-any.whl (66 kB)\n",
      "     ???????????????????????????????????????? 66.7/66.7 kB 10.9 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata<8,>=1.4\n",
      "  Downloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
      "Collecting tenacity<9,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting packaging<24,>=16.8\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "     ???????????????????????????????????????? 53.0/53.0 kB 9.8 MB/s eta 0:00:00\n",
      "Collecting pydeck<1,>=0.8.0b4\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "     ???????????????????????????????????????? 4.8/4.8 MB 114.1 MB/s eta 0:00:00\n",
      "Collecting python-dateutil<3,>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ?????????????????????????????????????? 247.7/247.7 kB 31.4 MB/s eta 0:00:00\n",
      "Collecting tzlocal<6,>=1.1\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Collecting cachetools<6,>=4.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting watchdog>=2.1.5\n",
      "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
      "     ???????????????????????????????????????? 83.0/83.0 kB 16.4 MB/s eta 0:00:00\n",
      "Collecting pillow<11,>=7.1.0\n",
      "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "     ???????????????????????????????????????? 4.5/4.5 MB 111.8 MB/s eta 0:00:00\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7\n",
      "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
      "     ?????????????????????????????????????? 195.4/195.4 kB 34.3 MB/s eta 0:00:00\n",
      "Collecting toml<2,>=0.10.1\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting click<9,>=7.0\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "     ???????????????????????????????????????? 97.9/97.9 kB 17.3 MB/s eta 0:00:00\n",
      "Collecting protobuf<5,>=3.20\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "     ?????????????????????????????????????? 294.6/294.6 kB 40.8 MB/s eta 0:00:00\n",
      "Collecting blinker<2,>=1.0.0\n",
      "  Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting altair<6,>=4.0\n",
      "  Downloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "     ?????????????????????????????????????? 996.9/996.9 kB 64.1 MB/s eta 0:00:00\n",
      "Collecting pyarrow>=7.0\n",
      "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "     ???????????????????????????????????????? 38.3/38.3 MB 58.3 MB/s eta 0:00:00\n",
      "Collecting rich<14,>=10.14.0\n",
      "  Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "     ?????????????????????????????????????? 240.6/240.6 kB 36.4 MB/s eta 0:00:00\n",
      "Collecting validators<1,>=0.2\n",
      "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Collecting typing-extensions<5,>=4.3.0\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Collecting tornado<7,>=6.0.3\n",
      "  Downloading tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "     ?????????????????????????????????????? 435.4/435.4 kB 49.6 MB/s eta 0:00:00\n",
      "Collecting requests<3,>=2.27\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "     ???????????????????????????????????????? 62.6/62.6 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "     ?????????????????????????????????????? 505.5/505.5 kB 51.5 MB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "     ?????????????????????????????????????? 345.4/345.4 kB 39.7 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "     ???????????????????????????????????????? 1.6/1.6 MB 95.9 MB/s eta 0:00:00\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "     ???????????????????????????????????????? 4.6/4.6 MB 114.3 MB/s eta 0:00:00\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "     ?????????????????????????????????????? 310.7/310.7 kB 40.5 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "     ?????????????????????????????????????? 103.1/103.1 kB 18.0 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "     ?????????????????????????????????????? 302.2/302.2 kB 40.1 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "     ???????????????????????????????????????? 38.4/38.4 MB 54.5 MB/s eta 0:00:00\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting jsonschema>=3.0\n",
      "  Downloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "     ???????????????????????????????????????? 85.5/85.5 kB 15.9 MB/s eta 0:00:00\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "     ?????????????????????????????????????? 133.2/133.2 kB 24.1 MB/s eta 0:00:00\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "     ???????????????????????????????????????? 56.1/56.1 kB 10.2 MB/s eta 0:00:00\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "     ???????????????????????????????????????? 62.7/62.7 kB 12.8 MB/s eta 0:00:00\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Collecting six>=1.5\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "     ?????????????????????????????????????? 142.1/142.1 kB 25.0 MB/s eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "     ?????????????????????????????????????? 121.1/121.1 kB 22.9 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "     ???????????????????????????????????????? 61.6/61.6 kB 11.4 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "     ?????????????????????????????????????? 163.8/163.8 kB 26.7 MB/s eta 0:00:00\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "     ???????????????????????????????????????? 87.5/87.5 kB 16.0 MB/s eta 0:00:00\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
      "     ???????????????????????????????????????? 1.2/1.2 MB 56.2 MB/s eta 0:00:00\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.33.0-py3-none-any.whl (26 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     ???????????????????????????????????????? 1.1/1.1 MB 81.9 MB/s eta 0:00:00\n",
      "Collecting attrs>=22.2.0\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "     ???????????????????????????????????????? 60.8/60.8 kB 11.2 MB/s eta 0:00:00\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, gurobipy, zipp, watchdog, validators, urllib3, tzlocal, tzdata, typing-extensions, tornado, toolz, toml, threadpoolctl, tenacity, smmap, six, rpds-py, pyparsing, pygments, protobuf, pillow, packaging, numpy, mdurl, MarkupSafe, kiwisolver, joblib, idna, fonttools, et-xmlfile, cycler, click, charset-normalizer, certifi, cachetools, blinker, attrs, scipy, requests, referencing, python-dateutil, pyarrow, plotly, openpyxl, markdown-it-py, jinja2, importlib-metadata, gunicorn, gitdb, contourpy, scikit-learn, rich, pydeck, pandas, matplotlib, jsonschema-specifications, gurobi-machinelearning, gitpython, seaborn, jsonschema, gurobipy-pandas, altair, streamlit\n",
      "Successfully installed MarkupSafe-2.1.5 altair-5.2.0 attrs-23.2.0 blinker-1.7.0 cachetools-5.3.2 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 contourpy-1.2.0 cycler-0.12.1 et-xmlfile-1.1.0 fonttools-4.49.0 gitdb-4.0.11 gitpython-3.1.42 gunicorn-21.2.0 gurobi-machinelearning-1.4.0 gurobipy-11.0.0 gurobipy-pandas-1.1.1 idna-3.6 importlib-metadata-7.0.1 jinja2-3.1.3 joblib-1.3.2 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 kiwisolver-1.4.5 markdown-it-py-3.0.0 matplotlib-3.8.3 mdurl-0.1.2 numpy-1.26.4 openpyxl-3.1.2 packaging-23.2 pandas-2.2.1 pillow-10.2.0 plotly-5.19.0 protobuf-4.25.3 pyarrow-15.0.0 pydeck-0.8.1b0 pygments-2.17.2 pyparsing-3.1.1 python-dateutil-2.8.2 pytz-2024.1 referencing-0.33.0 requests-2.31.0 rich-13.7.0 rpds-py-0.18.0 scikit-learn-1.4.1.post1 scipy-1.12.0 seaborn-0.13.2 six-1.16.0 smmap-5.0.1 streamlit-1.31.1 tenacity-8.2.3 threadpoolctl-3.3.0 toml-0.10.2 toolz-0.12.1 tornado-6.4 typing-extensions-4.10.0 tzdata-2024.1 tzlocal-5.2 urllib3-2.2.1 validators-0.22.0 watchdog-4.0.0 zipp-3.17.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 6777c72a535f\n",
      " ---> 3f0cb40d0b24\n",
      "Step 6/6 : ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8080\", \"--server.address=0.0.0.0\"]\n",
      " ---> Running in 7b9ff52f56ed\n",
      "Removing intermediate container 7b9ff52f56ed\n",
      " ---> 8d58955cdac6\n",
      "Successfully built 8d58955cdac6\n",
      "Successfully tagged us-east1-docker.pkg.dev/cmpc-innovation-cd4ml-test/repo-gurobi-optimization-bleaching-sf2-advanced/gurobi-optimization-bleaching-sf2-advanced:latest\n",
      "PUSH\n",
      "Pushing us-east1-docker.pkg.dev/cmpc-innovation-cd4ml-test/repo-gurobi-optimization-bleaching-sf2-advanced/gurobi-optimization-bleaching-sf2-advanced\n",
      "The push refers to repository [us-east1-docker.pkg.dev/cmpc-innovation-cd4ml-test/repo-gurobi-optimization-bleaching-sf2-advanced/gurobi-optimization-bleaching-sf2-advanced]\n",
      "620bc5937b5e: Preparing\n",
      "1d7777c81d6d: Preparing\n",
      "f2b7955d07d2: Preparing\n",
      "13e9fcf92c67: Preparing\n",
      "5b8a506fb91c: Preparing\n",
      "a6267a497621: Preparing\n",
      "84f540ade319: Preparing\n",
      "9fe4e8a1862c: Preparing\n",
      "909275a3eaaa: Preparing\n",
      "f3f47b3309ca: Preparing\n",
      "1a5fc1184c48: Preparing\n",
      "a6267a497621: Waiting\n",
      "84f540ade319: Waiting\n",
      "9fe4e8a1862c: Waiting\n",
      "909275a3eaaa: Waiting\n",
      "f3f47b3309ca: Waiting\n",
      "1a5fc1184c48: Waiting\n",
      "1d7777c81d6d: Pushed\n",
      "f2b7955d07d2: Pushed\n",
      "5b8a506fb91c: Pushed\n",
      "13e9fcf92c67: Pushed\n",
      "84f540ade319: Pushed\n",
      "a6267a497621: Pushed\n",
      "f3f47b3309ca: Pushed\n",
      "909275a3eaaa: Pushed\n",
      "1a5fc1184c48: Pushed\n",
      "9fe4e8a1862c: Pushed\n",
      "620bc5937b5e: Pushed\n",
      "latest: digest: sha256:a0a53f4aa12dceb197bd203704331604b2619d01dbfdbd3abac13fd4880951ec size: 2637\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                    IMAGES                                                                                                                                                   STATUS\n",
      "daff1f96-2892-4c8a-8a80-fb0804c8b19e  2024-02-26T03:22:03+00:00  2M36S     gs://cmpc-innovation-cd4ml-test_cloudbuild/source/1708917721.278808-38049e5654ca4763a6e8e8d4464a2e66.tgz  us-east1-docker.pkg.dev/cmpc-innovation-cd4ml-test/repo-gurobi-optimization-bleaching-sf2-advanced/gurobi-optimization-bleaching-sf2-advanced (+1 more)  SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 26 file(s) totalling 567.0 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://cmpc-innovation-cd4ml-test_cloudbuild/source/1708917721.278808-38049e5654ca4763a6e8e8d4464a2e66.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/cmpc-innovation-cd4ml-test/locations/global/builds/daff1f96-2892-4c8a-8a80-fb0804c8b19e].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/daff1f96-2892-4c8a-8a80-fb0804c8b19e?project=724348686027 ].\n"
     ]
    }
   ],
   "source": [
    "##### VERY IMPORTANT NOTATION\n",
    "\n",
    "# NOTE: The variable names in the gcloud command correspond to the variables defined in the configuration file\n",
    "#yaml\n",
    "\n",
    "# NOTE2: to pass the name of the variables (as always) you must use the dollar sign \"$\" but you must\n",
    "# to be enclosed in quotes (so that it is understood that it is the variable to be replaced in the configuration yaml)\n",
    "\n",
    "# NOTE3: it must be double quotes and without spaces to avoid problems\n",
    "\n",
    "! gcloud builds submit \\\n",
    "    --config=cloudbuild.yaml \\\n",
    "    --substitutions=_LOCATION=\"$REGION\",_REPOSITORY=\"$NAME_REPO\",_IMAGE=\"$NAME_IMAGE\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a446b9-1751-43fd-a0f0-82befe5b6c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28fba7d5",
   "metadata": {},
   "source": [
    "### Step 5: Deploy the artifact registry container image to cloud run\n",
    "You must run the gloud run deploy command (same as in the container registry) with the only difference being that it changes the location of the image, which is:\n",
    "\n",
    "  {LOCATION}-docker.pkg.dev/{PROJECT}/{REPOSITORY}/{IMAGE}/\n",
    " \n",
    " \n",
    "\n",
    "**IMPORTANT: DUE TO PERMISSIONS ISSUES, THE CLOUD RUN IS CONFIGURED SO THAT ANYONE WITH THE LINK CAN ACCESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51740ac6-8482-439b-bd23-0ce4a81edc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### como setear variables de ambiente en cloud run\n",
    "#--set-env-vars=PROJECT_GCP=$PROJECT_ID \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40f4016e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deploying container to Cloud Run service [gurobi-optimization-bleaching-sf2-advanced] in project [cmpc-innovation-cd4ml-test] region [us-east1]\n",
      "Deploying new service...\n",
      "Setting IAM Policy.......................done\n",
      "Creating Revision...............done\n",
      "Routing traffic.......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done\n",
      "Done.\n",
      "Service [gurobi-optimization-bleaching-sf2-advanced] revision [gurobi-optimization-bleaching-sf2-advanced-00001-rin] has been deployed and is serving 100 percent of traffic.\n",
      "Service URL: https://gurobi-optimization-bleaching-sf2-advanced-z33tzzez7a-ue.a.run.app\n"
     ]
    }
   ],
   "source": [
    "! gcloud run deploy $NAME_CLOUD_RUN \\\n",
    "    --image $REGION-docker.pkg.dev/$PROJECT_ID/$NAME_REPO/$NAME_IMAGE \\\n",
    "    --region $REGION \\\n",
    "    --set-env-vars=PROJECT_GCP=$PROJECT_ID \\\n",
    "    --allow-unauthenticated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722b3a0-c47c-42c0-8747-53cbc9af512c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b3789-6d92-4cb5-94c2-48586f71957e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77cffd3-a0b5-48a0-bc22-7fdbf00137eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
