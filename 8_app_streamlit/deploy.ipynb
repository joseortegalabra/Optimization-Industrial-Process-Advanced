{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea04d11-b156-4dff-ac80-08ab3e80c79b",
   "metadata": {},
   "source": [
    "# Deploy streamlit app into a cloud run (gcp)\n",
    "\n",
    "**Following the ideas of:**\n",
    "- deploy streamlit app in cloud run: https://medium.com/@faizififita1/how-to-deploy-your-streamlit-web-app-to-google-cloud-run-ba776487c5fe\n",
    "- deploy streamlit app into google app engine: https://dev.to/whitphx/how-to-deploy-streamlit-apps-to-google-app-engine-407o\n",
    "- deploy a flask app into a cloud run: my previous codes\n",
    "    - Create Dockerfile (an example Dockerfile can be found at: https://firebase.google.com/docs/hosting/cloud-run?hl=es-419#python)\n",
    "\n",
    "\n",
    "**Steps:**\n",
    "- Create a sript with codes\n",
    "- Create a .env file with environment variables that you don't want to expose\n",
    "- Define the parameters to deploy\n",
    "- Run codes that do diferents steps to deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc770c07-ce2a-492f-97ac-bd9fb8aaa545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03c6054b-c6e1-4a49-b788-156a92e3a057",
   "metadata": {},
   "source": [
    "## I) INTRODUCTION\n",
    "\n",
    "These codes could be run in the Google SDK console, as well as run on a notebook.\n",
    "For a data scientist who is not specialized in devops practices, using a notebook is more intuitive to use\n",
    "\n",
    "#### Previous steps\n",
    "- Stop in the root folder of the application (this notebook is created in the root)\n",
    "- Have scripts created to contain and upload to cloud run\n",
    "\n",
    "#### Important information\n",
    "**To run a console command in a Jupyter notebook and the python variables stored in the notebook can also be passed to the command, you must use the peso sign ($) and not use the assignment command (=)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c5998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646304fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef96bd-3a6c-48df-8078-6efaf5b6c09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bf9ae26",
   "metadata": {},
   "source": [
    "## II) INITIALIZE READ .ENV WITH ENV VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "338a3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv # package used in jupyter notebook to read the variables in file .env\n",
    "\n",
    "\"\"\" get env variable from .env \"\"\"\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "\"\"\" Read env variables and save it as python variable \"\"\"\n",
    "WLSACCESSID = os.environ.get(\"WLSACCESSID\", \"\")\n",
    "WLSSECRET = os.environ.get(\"WLSSECRET\", \"\")\n",
    "LICENSEID = int(os.environ.get(\"LICENSEID\", \"\"))\n",
    "PROJECT_GCP = os.environ.get(\"PROJECT_GCP\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e91a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c42cc73c-8063-4e9f-96c7-ae5343c55c53",
   "metadata": {},
   "source": [
    "## III) DEFINE PARAMETERS TO DEPLOY APP INTO A CLOUD RUN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4bd3e5-8975-45c0-abbc-a5eea58acfd0",
   "metadata": {},
   "source": [
    "### Step 0: Connect to GCP project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d1563e-5625-4963-952d-4c6468d261c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You do not appear to have access to project [$PROJECT_ID] or it does not exist.\n",
      "ERROR: (gcloud.config.set) The project property must be set to a valid project ID, [$PROJECT_ID] is not a valid project ID.\n",
      "To set your project, run:\n",
      "\n",
      "  $ gcloud config set project PROJECT_ID\n",
      "\n",
      "or to unset it, run:\n",
      "\n",
      "  $ gcloud config unset project\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db232b7-2239-4ccc-b8f4-7c313b09cd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d690650-e360-4d14-9c69-e792e32eb0f1",
   "metadata": {},
   "source": [
    "### Step 1: Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90fe41fe-0fdc-4999-b2f8-7e225cfdb5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARARAMETERS\n",
    "\n",
    "# general gcp\n",
    "REGION = 'us-east1'\n",
    "\n",
    "# name of the repo in artifact registry where will be saved docker images\n",
    "NAME_REPO = 'repo-gurobi-optimization-bleaching-sf2-advanced'\n",
    "FORMAT_REPO = 'docker'\n",
    "DESCRIPTION_REPO = \"repo web app con modelo de optimizaci√≥n de gurobi sf2 advanced\"\n",
    "\n",
    "# name of the docker image saved in docker repo in artifact registry\n",
    "NAME_IMAGE = 'gurobi-optimization-bleaching-sf2-advanced'\n",
    "\n",
    "# name of cloud run where the app web will be located\n",
    "NAME_CLOUD_RUN = 'gurobi-optimization-bleaching-sf2-advanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839eb1d7-f6ba-4d98-ba9b-2490e1cc66a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0591b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4eda8-348f-439f-9413-adba04c635ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62e1c63e",
   "metadata": {},
   "source": [
    "# IV) Upload a docker image with the codes of the app in Artifact Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f0053-ca26-44cf-ac37-de8928fe4bc9",
   "metadata": {},
   "source": [
    "- Artifact registry is the replacement for container registry and recommended by google. The only difference is that the image is saved in this new service and you need to run another command\n",
    "\n",
    "- **Additionally, every time a new image is uploaded to the artifact registry (rerun the corresponding gcloud command), it receives the latest tag and is the one used to create/update the created cloud run**\n",
    "\n",
    "- Uploading the image to the artifact Registry requires more steps than uploading it to the container registry\n",
    "\n",
    "- Cloud build integration documentation with Artifact Registry: https://cloud.google.com/artifact-registry/docs/configure-cloud-build?hl=es-419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072af20-fc4b-4bd0-87f3-eac65495a710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fccab59b-8d26-4e56-ac6f-61d71014890d",
   "metadata": {},
   "source": [
    "### Step 1. Create repository in artifact registry (if it does not exist)\n",
    "- Unlike container registry which was automatic, in artifact registry you have to create it. **If the repo already exists the gcloud command return an error but doesn't stop de execution of the notebook**\n",
    "\n",
    "- A repo is created which can have multiple images and each one have different versions\n",
    "\n",
    "- Documentation: create repo in artifact registry: https://cloud.google.com/artifact-registry/docs/repositories/create-repos#gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceae1f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "# create repo artifact registry\n",
    "! gcloud artifacts repositories create $NAME_REPO \\\n",
    "--repository-format $FORMAT_REPO \\\n",
    "--location $REGION \\\n",
    "--description \"$DESCRIPTION_REPO\" \\\n",
    "--async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040f26c-e1a6-400d-a358-5502c822f932",
   "metadata": {},
   "source": [
    "### Step 2: Set up a Docker build\n",
    "\n",
    "It is necessary to create a **yaml** with the configuration to build the docker image in Artifact Registry.\n",
    "\n",
    "It has the following form\n",
    "\n",
    "<code>\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "   args: [ 'build', '-t', '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}', '.' ]\n",
    "images:\n",
    "- '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}'\n",
    "<code>\n",
    "    \n",
    "\n",
    "**This is a GENERIC FILE that can be recycled because it is parameterized to work with any docker repo in the artifact registry**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dcace8-1a0f-4833-89ab-544bfc2aa7c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f13b0ae",
   "metadata": {},
   "source": [
    "--> Running the following line of code creates a yaml file with the desired configuration.\n",
    "\n",
    "Documentation: https://stackabuse.com/reading-and-writing-yaml-to-a-file-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98471485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# create a python diccionary with the content of the yaml cloudbuild generic\n",
    "\n",
    "dict_python_yaml_cloudbuild = {'steps': [{'name': 'gcr.io/cloud-builders/docker',\n",
    "   'args': ['build', '-t', '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}', '.']}],\n",
    " 'images': ['${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}']}\n",
    "\n",
    "# save dicctionary in yaml format\n",
    "with open(r'cloudbuild.yaml', 'w') as file:\n",
    "    documents = yaml.dump(dict_python_yaml_cloudbuild, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e475e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b267e53",
   "metadata": {},
   "source": [
    "### Step 3: Create Dockerfile\n",
    "Like the previous step, in this case you need to create the Dockerfile to be able to upload the image to the Artifact Registry. Typically, the dockerfile is created manually.\n",
    "\n",
    "In this example, the content of the dockerfile is defined within a parameterized string, which allows it to be a transversal file and applicable to any deploy in a cloud run\n",
    "\n",
    "Generally, most of the time, **the Dockerfile does not need to be modified unless you want to change the python vers**ion. So by running the following code you obtain the Dockerfile of the web app\n",
    "\n",
    "https://prnt.sc/uwBOFChK8QU8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0e2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A STRING THAT REPRESENT THE DOCKER FILE\n",
    "\n",
    "#old version with requirements file inside the script\n",
    "# string_dockerfile = '''\n",
    "# FROM python:3.10\n",
    "# EXPOSE 8080\n",
    "# WORKDIR /app\n",
    "# COPY . ./\n",
    "# RUN pip install streamlit gunicorn\n",
    "# ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8080\", \"--server.address=0.0.0.0\"]\n",
    "# '''\n",
    "\n",
    "\n",
    "string_dockerfile = '''\n",
    "FROM python:3.10\n",
    "EXPOSE 8080\n",
    "WORKDIR /app\n",
    "COPY . ./\n",
    "RUN pip install -r requirements.txt\n",
    "ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8080\", \"--server.address=0.0.0.0\"]\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc57cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar dockerfile\n",
    "with open('Dockerfile', 'w') as file:\n",
    "    file.write(string_dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b43e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15903a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba335ac5",
   "metadata": {},
   "source": [
    "### Step 4: Containerize (docker image) web app codes using cloud build and upload them to artifact registry\n",
    "- In this step, a docker image is created with the necessary codes for the web app and then this image is uploaded to Artifact Registry (using as a base the \"cloudbuild.yaml\" file that calls the \"Dockerfile\", created in the steps previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f993074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"3a3f5975-6c79-42a3-8128-6178ed601fbb\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://cmpc-innovation-cd4ml-test_cloudbuild/source/1709119445.339416-4df51c3cebbe4a208164c28edb21e640.tgz#1709119447136715\n",
      "Copying gs://cmpc-innovation-cd4ml-test_cloudbuild/source/1709119445.339416-4df51c3cebbe4a208164c28edb21e640.tgz#1709119447136715...\n",
      "/ [0 files][    0.0 B/222.6 KiB]                                                \n",
      "/ [1 files][222.6 KiB/222.6 KiB]                                                \n",
      "\n",
      "Operation completed over 1 objects/222.6 KiB.                                    \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  817.2kB\n",
      "\n",
      "\n",
      "Step 1/6 : FROM python:3.10\n",
      "3.10: Pulling from library/python\n",
      "7bb465c29149: Already exists\n",
      "2b9b41aaa3c5: Already exists\n",
      "49b40be4436e: Already exists\n",
      "c558fac597f8: Already exists\n",
      "11402150a57e: Already exists\n",
      "78d5a3440207: Pulling fs layer\n",
      "833a11e16f36: Pulling fs layer\n",
      "e81a4b56efff: Pulling fs layer\n",
      "833a11e16f36: Verifying Checksum\n",
      "833a11e16f36: Download complete\n",
      "78d5a3440207: Verifying Checksum\n",
      "78d5a3440207: Download complete\n",
      "e81a4b56efff: Verifying Checksum\n",
      "e81a4b56efff: Download complete\n",
      "78d5a3440207: Pull complete\n",
      "833a11e16f36: Pull complete\n",
      "e81a4b56efff: Pull complete\n",
      "Digest: sha256:b54e76c629a98430ac9c92e4f6bddeb672396a895b44a85022d12ee2f7239144\n",
      "Status: Downloaded newer image for python:3.10\n",
      " ---> 22546fe66182\n",
      "Step 2/6 : EXPOSE 8080\n",
      " ---> Running in 8fa6153d712f\n",
      "Removing intermediate container 8fa6153d712f\n",
      " ---> f646c7a837da\n",
      "Step 3/6 : WORKDIR /app\n",
      " ---> Running in 375de706cacc\n",
      "Removing intermediate container 375de706cacc\n",
      " ---> 0d51acc5f6b6\n",
      "Step 4/6 : COPY . ./\n",
      " ---> 55951c472dcc\n",
      "Step 5/6 : RUN pip install -r requirements.txt\n",
      " ---> Running in 974497eb8143\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.31.1-py2.py3-none-any.whl (8.4 MB)\n",
      "     ???????????????????????????????????????? 8.4/8.4 MB 26.7 MB/s eta 0:00:00\n",
      "Collecting gunicorn\n",
      "  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
      "     ???????????????????????????????????????? 80.2/80.2 kB 13.8 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "     ???????????????????????????????????????? 13.0/13.0 MB 64.9 MB/s eta 0:00:00\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "     ???????????????????????????????????????? 11.6/11.6 MB 79.9 MB/s eta 0:00:00\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "     ?????????????????????????????????????? 294.9/294.9 kB 35.4 MB/s eta 0:00:00\n",
      "Collecting plotly\n",
      "  Downloading plotly-5.19.0-py3-none-any.whl (15.7 MB)\n",
      "     ???????????????????????????????????????? 15.7/15.7 MB 68.3 MB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "     ???????????????????????????????????????? 18.2/18.2 MB 67.2 MB/s eta 0:00:00\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "     ???????????????????????????????????????? 12.1/12.1 MB 81.1 MB/s eta 0:00:00\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "     ?????????????????????????????????????? 250.0/250.0 kB 34.5 MB/s eta 0:00:00\n",
      "Collecting gurobipy\n",
      "  Downloading gurobipy-11.0.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (13.4 MB)\n",
      "     ???????????????????????????????????????? 13.4/13.4 MB 80.1 MB/s eta 0:00:00\n",
      "Collecting gurobipy-pandas\n",
      "  Downloading gurobipy_pandas-1.1.1-py3-none-any.whl (19 kB)\n",
      "Collecting gurobi-machinelearning\n",
      "  Downloading gurobi_machinelearning-1.4.0-py3-none-any.whl (66 kB)\n",
      "     ???????????????????????????????????????? 66.7/66.7 kB 11.2 MB/s eta 0:00:00\n",
      "Collecting blinker<2,>=1.0.0\n",
      "  Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting toml<2,>=0.10.1\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting pyarrow>=7.0\n",
      "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "     ???????????????????????????????????????? 38.3/38.3 MB 42.6 MB/s eta 0:00:00\n",
      "Collecting validators<1,>=0.2\n",
      "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Collecting click<9,>=7.0\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "     ???????????????????????????????????????? 97.9/97.9 kB 15.4 MB/s eta 0:00:00\n",
      "Collecting pydeck<1,>=0.8.0b4\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "     ???????????????????????????????????????? 4.8/4.8 MB 80.2 MB/s eta 0:00:00\n",
      "Collecting tenacity<9,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting altair<6,>=4.0\n",
      "  Downloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "     ?????????????????????????????????????? 996.9/996.9 kB 64.1 MB/s eta 0:00:00\n",
      "Collecting watchdog>=2.1.5\n",
      "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
      "     ???????????????????????????????????????? 83.0/83.0 kB 14.8 MB/s eta 0:00:00\n",
      "Collecting protobuf<5,>=3.20\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "     ?????????????????????????????????????? 294.6/294.6 kB 36.3 MB/s eta 0:00:00\n",
      "Collecting python-dateutil<3,>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ?????????????????????????????????????? 247.7/247.7 kB 35.4 MB/s eta 0:00:00\n",
      "Collecting tornado<7,>=6.0.3\n",
      "  Downloading tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "     ?????????????????????????????????????? 435.4/435.4 kB 45.5 MB/s eta 0:00:00\n",
      "Collecting cachetools<6,>=4.0\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting tzlocal<6,>=1.1\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Collecting packaging<24,>=16.8\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "     ???????????????????????????????????????? 53.0/53.0 kB 9.1 MB/s eta 0:00:00\n",
      "Collecting typing-extensions<5,>=4.3.0\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7\n",
      "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
      "     ?????????????????????????????????????? 195.4/195.4 kB 27.1 MB/s eta 0:00:00\n",
      "Collecting rich<14,>=10.14.0\n",
      "  Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "     ?????????????????????????????????????? 240.6/240.6 kB 33.8 MB/s eta 0:00:00\n",
      "Collecting pillow<11,>=7.1.0\n",
      "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "     ???????????????????????????????????????? 4.5/4.5 MB 83.8 MB/s eta 0:00:00\n",
      "Collecting requests<3,>=2.27\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "     ???????????????????????????????????????? 62.6/62.6 kB 10.5 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata<8,>=1.4\n",
      "  Downloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "     ?????????????????????????????????????? 345.4/345.4 kB 37.9 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "     ?????????????????????????????????????? 505.5/505.5 kB 43.9 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "     ???????????????????????????????????????? 1.6/1.6 MB 77.1 MB/s eta 0:00:00\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "     ???????????????????????????????????????? 4.6/4.6 MB 84.9 MB/s eta 0:00:00\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "     ?????????????????????????????????????? 310.7/310.7 kB 35.6 MB/s eta 0:00:00\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "     ?????????????????????????????????????? 103.1/103.1 kB 16.5 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "     ???????????????????????????????????????? 38.4/38.4 MB 42.8 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "     ?????????????????????????????????????? 302.2/302.2 kB 34.1 MB/s eta 0:00:00\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting jsonschema>=3.0\n",
      "  Downloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "     ???????????????????????????????????????? 85.5/85.5 kB 12.8 MB/s eta 0:00:00\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "     ???????????????????????????????????????? 56.1/56.1 kB 8.7 MB/s eta 0:00:00\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "     ?????????????????????????????????????? 133.2/133.2 kB 19.1 MB/s eta 0:00:00\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "     ???????????????????????????????????????? 62.7/62.7 kB 10.2 MB/s eta 0:00:00\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Collecting six>=1.5\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "     ?????????????????????????????????????? 121.1/121.1 kB 20.2 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "     ???????????????????????????????????????? 61.6/61.6 kB 10.5 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "     ?????????????????????????????????????? 163.8/163.8 kB 24.9 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "     ?????????????????????????????????????? 142.1/142.1 kB 24.0 MB/s eta 0:00:00\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
      "     ???????????????????????????????????????? 1.2/1.2 MB 64.5 MB/s eta 0:00:00\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "     ???????????????????????????????????????? 87.5/87.5 kB 12.6 MB/s eta 0:00:00\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     ???????????????????????????????????????? 1.1/1.1 MB 66.2 MB/s eta 0:00:00\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Collecting attrs>=22.2.0\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "     ???????????????????????????????????????? 60.8/60.8 kB 10.1 MB/s eta 0:00:00\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.33.0-py3-none-any.whl (26 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, gurobipy, zipp, watchdog, validators, urllib3, tzlocal, tzdata, typing-extensions, tornado, toolz, toml, threadpoolctl, tenacity, smmap, six, rpds-py, pyparsing, pygments, protobuf, pillow, packaging, numpy, mdurl, MarkupSafe, kiwisolver, joblib, idna, fonttools, et-xmlfile, cycler, click, charset-normalizer, certifi, cachetools, blinker, attrs, scipy, requests, referencing, python-dateutil, pyarrow, plotly, openpyxl, markdown-it-py, jinja2, importlib-metadata, gunicorn, gitdb, contourpy, scikit-learn, rich, pydeck, pandas, matplotlib, jsonschema-specifications, gurobi-machinelearning, gitpython, seaborn, jsonschema, gurobipy-pandas, altair, streamlit\n",
      "Successfully installed MarkupSafe-2.1.5 altair-5.2.0 attrs-23.2.0 blinker-1.7.0 cachetools-5.3.3 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 contourpy-1.2.0 cycler-0.12.1 et-xmlfile-1.1.0 fonttools-4.49.0 gitdb-4.0.11 gitpython-3.1.42 gunicorn-21.2.0 gurobi-machinelearning-1.4.0 gurobipy-11.0.0 gurobipy-pandas-1.1.1 idna-3.6 importlib-metadata-7.0.1 jinja2-3.1.3 joblib-1.3.2 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 kiwisolver-1.4.5 markdown-it-py-3.0.0 matplotlib-3.8.3 mdurl-0.1.2 numpy-1.26.4 openpyxl-3.1.2 packaging-23.2 pandas-2.2.1 pillow-10.2.0 plotly-5.19.0 protobuf-4.25.3 pyarrow-15.0.0 pydeck-0.8.1b0 pygments-2.17.2 pyparsing-3.1.1 python-dateutil-2.8.2 pytz-2024.1 referencing-0.33.0 requests-2.31.0 rich-13.7.0 rpds-py-0.18.0 scikit-learn-1.4.1.post1 scipy-1.12.0 seaborn-0.13.2 six-1.16.0 smmap-5.0.1 streamlit-1.31.1 tenacity-8.2.3 threadpoolctl-3.3.0 toml-0.10.2 toolz-0.12.1 tornado-6.4 typing-extensions-4.10.0 tzdata-2024.1 tzlocal-5.2 urllib3-2.2.1 validators-0.22.0 watchdog-4.0.0 zipp-3.17.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 974497eb8143\n",
      " ---> bb08fec4389e\n",
      "Step 6/6 : ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8080\", \"--server.address=0.0.0.0\"]\n",
      " ---> Running in 7d5d81481ee5\n",
      "Removing intermediate container 7d5d81481ee5\n",
      " ---> 6c61c45e2812\n",
      "Successfully built 6c61c45e2812\n",
      "Successfully tagged us-east1-docker.pkg.dev/cmpc-innovation-cd4ml-test/repo-gurobi-optimization-bleaching-sf2-advanced/gurobi-optimization-bleaching-sf2-advanced:latest\n",
      "PUSH\n",
      "Pushing us-east1-docker.pkg.dev/cmpc-innovation-cd4ml-test/repo-gurobi-optimization-bleaching-sf2-advanced/gurobi-optimization-bleaching-sf2-advanced\n",
      "The push refers to repository [us-east1-docker.pkg.dev/cmpc-innovation-cd4ml-test/repo-gurobi-optimization-bleaching-sf2-advanced/gurobi-optimization-bleaching-sf2-advanced]\n",
      "1adfcfc05b87: Preparing\n",
      "b1b1c563fcbf: Preparing\n",
      "d904197dbac6: Preparing\n",
      "13e9fcf92c67: Preparing\n",
      "5b8a506fb91c: Preparing\n",
      "a6267a497621: Preparing\n",
      "84f540ade319: Preparing\n",
      "9fe4e8a1862c: Preparing\n",
      "909275a3eaaa: Preparing\n",
      "f3f47b3309ca: Preparing\n",
      "1a5fc1184c48: Preparing\n",
      "a6267a497621: Waiting\n",
      "84f540ade319: Waiting\n",
      "9fe4e8a1862c: Waiting\n",
      "909275a3eaaa: Waiting\n",
      "f3f47b3309ca: Waiting\n",
      "1a5fc1184c48: Waiting\n",
      "13e9fcf92c67: Layer already exists\n",
      "5b8a506fb91c: Layer already exists\n",
      "a6267a497621: Layer already exists\n",
      "84f540ade319: Layer already exists\n",
      "9fe4e8a1862c: Layer already exists\n",
      "909275a3eaaa: Layer already exists\n",
      "f3f47b3309ca: Layer already exists\n",
      "1a5fc1184c48: Layer already exists\n",
      "d904197dbac6: Pushed\n",
      "b1b1c563fcbf: Pushed\n",
      "1adfcfc05b87: Pushed\n",
      "latest: digest: sha256:0a14d00103bf5b44b4f159a1c8c0025419a334cbf2cd1aefd3715cac66d6a067 size: 2637\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                    IMAGES                                                                                                                                                   STATUS\n",
      "3a3f5975-6c79-42a3-8128-6178ed601fbb  2024-02-28T11:24:08+00:00  2M41S     gs://cmpc-innovation-cd4ml-test_cloudbuild/source/1709119445.339416-4df51c3cebbe4a208164c28edb21e640.tgz  us-east1-docker.pkg.dev/cmpc-innovation-cd4ml-test/repo-gurobi-optimization-bleaching-sf2-advanced/gurobi-optimization-bleaching-sf2-advanced (+1 more)  SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 38 file(s) totalling 760.8 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://cmpc-innovation-cd4ml-test_cloudbuild/source/1709119445.339416-4df51c3cebbe4a208164c28edb21e640.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/cmpc-innovation-cd4ml-test/locations/global/builds/3a3f5975-6c79-42a3-8128-6178ed601fbb].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/3a3f5975-6c79-42a3-8128-6178ed601fbb?project=724348686027 ].\n"
     ]
    }
   ],
   "source": [
    "##### VERY IMPORTANT NOTATION\n",
    "\n",
    "# NOTE: The variable names in the gcloud command correspond to the variables defined in the configuration file\n",
    "#yaml\n",
    "\n",
    "# NOTE2: to pass the name of the variables (as always) you must use the dollar sign \"$\" but you must\n",
    "# to be enclosed in quotes (so that it is understood that it is the variable to be replaced in the configuration yaml)\n",
    "\n",
    "# NOTE3: it must be double quotes and without spaces to avoid problems\n",
    "\n",
    "! gcloud builds submit \\\n",
    "    --config=cloudbuild.yaml \\\n",
    "    --substitutions=_LOCATION=\"$REGION\",_REPOSITORY=\"$NAME_REPO\",_IMAGE=\"$NAME_IMAGE\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a446b9-1751-43fd-a0f0-82befe5b6c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28fba7d5",
   "metadata": {},
   "source": [
    "### Step 5: Deploy the artifact registry container image to cloud run\n",
    "You must run the gloud run deploy command (same as in the container registry) with the only difference being that it changes the location of the image, which is:\n",
    "\n",
    "  {LOCATION}-docker.pkg.dev/{PROJECT}/{REPOSITORY}/{IMAGE}/\n",
    " \n",
    " \n",
    "\n",
    "**IMPORTANT: DUE TO PERMISSIONS ISSUES, THE CLOUD RUN IS CONFIGURED SO THAT ANYONE WITH THE LINK CAN ACCESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51740ac6-8482-439b-bd23-0ce4a81edc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### como setear variables de ambiente en cloud run\n",
    "#--set-env-vars=PROJECT_GCP=$PROJECT_ID \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40f4016e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (gcloud.run.deploy) unrecognized arguments:\n",
      "  #--set-env-vars=WLSACCESSID=$WLSACCESSID (did you mean '--set-env-vars'?)\n",
      "  #--set-env-vars=WLSSECRET=$WLSSECRET (did you mean '--set-env-vars'?)\n",
      "  #--set-env-vars=LICENSEID=$LICENSEID (did you mean '--set-env-vars'?)\n",
      "  To search the help text of gcloud commands, run:\n",
      "  gcloud help -- SEARCH_TERMS\n"
     ]
    }
   ],
   "source": [
    "! gcloud run deploy $NAME_CLOUD_RUN \\\n",
    "    --image $REGION-docker.pkg.dev/$PROJECT_ID/$NAME_REPO/$NAME_IMAGE \\\n",
    "    --region $REGION \\\n",
    "    --set-env-vars=PROJECT_GCP=$PROJECT_GCP \\\n",
    "    --set-env-vars=WLSACCESSID=$WLSACCESSID \\\n",
    "    --set-env-vars=WLSSECRET=$WLSSECRET \\\n",
    "    --set-env-vars=LICENSEID=$LICENSEID \\\n",
    "    --allow-unauthenticated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b722b3a0-c47c-42c0-8747-53cbc9af512c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (gcloud.run.deploy) Invalid resource name [$NAME_CLOUD_RUN]. The name must use only lowercase alphanumeric characters and dashes, cannot begin or end with a dash, and cannot be longer than 63 characters.\n"
     ]
    }
   ],
   "source": [
    "! gcloud run deploy $NAME_CLOUD_RUN \\\n",
    "    --image $REGION-docker.pkg.dev/$PROJECT_ID/$NAME_REPO/$NAME_IMAGE \\\n",
    "    --region $REGION \\\n",
    "    --allow-unauthenticated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b45b3789-6d92-4cb5-94c2-48586f71957e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gurobi-optimization-bleaching-sf2-advanced'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAME_CLOUD_RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a6f355-1429-4bb5-8c37-823f71ffe4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
